{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd7d9625-18f9-4780-939b-abfcce550df3"
      },
      "source": [
        "## Importing Libraries"
      ],
      "id": "dd7d9625-18f9-4780-939b-abfcce550df3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "036f4f2c-5bbe-4dd5-af1b-c8a5bde51ee6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from IPython import display\n",
        "print('Tensorflow version:', tf.__version__)"
      ],
      "id": "036f4f2c-5bbe-4dd5-af1b-c8a5bde51ee6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69afaa09-606d-4342-9538-8432c7ffc8a9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show(images, n_cols=None):\n",
        "    n_cols = n_cols or len(images)\n",
        "    n_rows = (len(images) - 1) // n_cols + 1\n",
        "    if images.shape[-1] == 1:\n",
        "        images = np.reshape(images,[images.shape[0],images.shape[1],images.shape[2]])\n",
        "        #images = np.squeeze(images, axis=-1)\n",
        "    plt.figure(figsize=(n_cols, n_rows))\n",
        "    for index, image in enumerate(images):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.axis(\"on\")"
      ],
      "id": "69afaa09-606d-4342-9538-8432c7ffc8a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey1o4o7URTGd"
      },
      "source": [
        "## The Fashion MNIST Data preprocessing"
      ],
      "id": "Ey1o4o7URTGd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e5fc9da-5653-45fa-9740-e6e719e971c5"
      },
      "outputs": [],
      "source": [
        "#Fashion MNIST (generator and discriminator)\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# images pixels 0-255 and we need to standardize it\n",
        "\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0"
      ],
      "id": "3e5fc9da-5653-45fa-9740-e6e719e971c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69e6f379-f319-4974-8e1c-e2ada3596492"
      },
      "source": [
        "## Create Batches of Training Data"
      ],
      "id": "69e6f379-f319-4974-8e1c-e2ada3596492"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94763d84-7f49-4415-8d2d-048195ad3e5a"
      },
      "outputs": [],
      "source": [
        "#visualize some images 5 x 5 grid images using Matplotlib\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i], cmap=plt.cm.binary) # for gray scale\n",
        "plt.show()\n",
        "\n",
        "#Tensorflow Data creation\n",
        "#Creating batches and shuffling data as it can fit in any batch of tensors\n",
        "\n",
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(1000)\n",
        "\n",
        "#After creating the dataset, it is batched into sets of 32 elements (batch_size) with drop_remainder=True (meaning any remaining elements less than the batch size are dropped).\n",
        "#Additionally, prefetch(1) is used to prefetch one batch of data, which can help to overlap data preprocessing and model execution.\n",
        "\n",
        "dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)"
      ],
      "id": "94763d84-7f49-4415-8d2d-048195ad3e5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7219bc-7d5e-42b4-90b2-1b477828fec8"
      },
      "source": [
        "## Build the Generator Network for DCGAN"
      ],
      "id": "7a7219bc-7d5e-42b4-90b2-1b477828fec8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f260911-a04c-473e-b268-f68af33b0770",
        "outputId": "aa6833da-8a4f-4269-a15f-4f15637b45a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 12544)             1266944   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 7, 7, 256)         1024      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTr  (None, 7, 7, 128)         819328    \n",
            " anspose)                                                        \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 7, 7, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 14, 14, 64)        204864    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 14, 14, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2D  (None, 28, 28, 1)         1601      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2294529 (8.75 MB)\n",
            "Trainable params: 2293633 (8.75 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_features = 100 # >30 (depending upon compute power available)keras.layer\n",
        "\n",
        "#generator model to reach 28*28*1\n",
        "\n",
        "generator = keras.models.Sequential([\n",
        "    keras.layers.Dense(7 * 7 * 256,input_shape=[num_features]), #small DCGAN\n",
        "    keras.layers.Reshape([7,7,256]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(128,(5,5),(1,1),padding='same',activation='selu'), #selu activation and betetr for DCGANs. A fully connected layer\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(64,(5,5),(2,2),padding='same',activation='selu'), #selu activation and betetr for DCGANs. A fully connected layer\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(1,(5,5),(2,2),padding='same',activation='tanh') # A fully connected layer\n",
        "])\n",
        "\n",
        "generator.summary()"
      ],
      "id": "4f260911-a04c-473e-b268-f68af33b0770"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa2e2452-a207-4380-8c75-26ddb6f335de"
      },
      "outputs": [],
      "source": [
        "# vector of random noise without being trained\n",
        "\n",
        "noise = tf.random.normal(shape=[1,num_features])\n",
        "\n",
        "generated_image = generator(noise,training=False)\n",
        "show(generated_image,1) #plotting one such generated image"
      ],
      "id": "aa2e2452-a207-4380-8c75-26ddb6f335de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21b1583c-6909-4f01-b28e-b592d4857d7a"
      },
      "source": [
        "## Build the Discriminator Network for DCGAN"
      ],
      "id": "21b1583c-6909-4f01-b28e-b592d4857d7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "277800c6-b1d0-4b50-bcdd-9de32eb218b1",
        "outputId": "93d18757-9239-4caa-d218-f391ee7d62ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 14, 14, 64)        1664      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 7, 7, 128)         204928    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 256)         819456    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12544)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 12545     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1038593 (3.96 MB)\n",
            "Trainable params: 1038593 (3.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#discriminator model using CNN\n",
        "\n",
        "discriminator = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(64, (5,5),(2,2),padding='same',input_shape=[28,28,1]),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dropout(0.3),\n",
        "\n",
        "    keras.layers.Conv2D(128,(5,5),(2,2),padding='same'),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dropout(0.3),\n",
        "\n",
        "    keras.layers.Conv2D(256,(5,5),(1,1),padding='same'),\n",
        "    keras.layers.LeakyReLU(0.2),\n",
        "    keras.layers.Dropout(0.3),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "\n",
        "discriminator.summary()"
      ],
      "id": "277800c6-b1d0-4b50-bcdd-9de32eb218b1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c25639b0-5352-4317-a2a7-aaef7dab4c46",
        "outputId": "19445752-11c1-4356-e27c-34095d685c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.5003049]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "decision = discriminator(generated_image) #check discriminator on random noise image\n",
        "\n",
        "print(decision) #value between [0,1]"
      ],
      "id": "c25639b0-5352-4317-a2a7-aaef7dab4c46"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCv-6k7Rhp73"
      },
      "source": [
        "## Compile the dcgan"
      ],
      "id": "oCv-6k7Rhp73"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76c9b224-94fb-40ea-b624-7502e6cf8520"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
        "d_loss = BinaryCrossentropy()\n",
        "\n",
        "discriminator.trainable = False #to train generator and discriminator separately\n",
        "\n",
        "gan = keras.models.Sequential([generator,discriminator])\n",
        "gan.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "g_loss = BinaryCrossentropy()\n",
        "\n"
      ],
      "id": "76c9b224-94fb-40ea-b624-7502e6cf8520"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c678ef7-7651-4d3c-9f8b-3ca9ef52f9f9"
      },
      "outputs": [],
      "source": [
        "seed = tf.random.normal(shape=[batch_size,100]) # seed image"
      ],
      "id": "9c678ef7-7651-4d3c-9f8b-3ca9ef52f9f9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7882f622-d454-4d99-9ead-fc8e270c5a96"
      },
      "outputs": [],
      "source": [
        "## Source https://www.tensorflow.org/tutorials/generative/dcgan#create_a_gif\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training=False)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "    for i in range(25):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='binary')\n",
        "        plt.axis('off')\n",
        "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()"
      ],
      "id": "7882f622-d454-4d99-9ead-fc8e270c5a96"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Training Procedure"
      ],
      "metadata": {
        "id": "9a-TDNGvX9dw"
      },
      "id": "9a-TDNGvX9dw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e670cf8-a513-4e8f-89f6-53960513be89"
      },
      "outputs": [],
      "source": [
        "# for each iteration, we will train first discriminator and then generator. For discriminator\n",
        "# we will sample mini batches for discriminator from training data. Then we obtain fake images\n",
        "# from random noise and in generator and then compare real images with discriminator\n",
        "# We will pick gradient step for mini batches for fake and real images and update the discriminator\n",
        "\n",
        "def train_dcgan(gan,dataset,batch_size, num_features, epochs=5):\n",
        "    generator, discriminator = gan.layers\n",
        "    for epoch in tqdm(range(epochs)): #iteration for each epoch with tqdm - distribution of every epoch\n",
        "        print('Epochs {}/{}'.format(epoch+1, epochs))\n",
        "        for X_batch in dataset: #iteration for batches in dataset\n",
        "            #train generator a bit\n",
        "            noise = tf.random.normal(shape=[batch_size,num_features])\n",
        "            generated_images = generator(noise) #pass random noise to generated to get generated image (FAKE)\n",
        "\n",
        "            X_fake_and_real = tf.concat([generated_images, X_batch],axis=0)\n",
        "            #class labels specify\n",
        "            y1 = tf.constant([[0.]]*batch_size +[[1.]]*batch_size)\n",
        "\n",
        "            discriminator.trainable = True #train discriminator\n",
        "            discriminator.train_on_batch(X_fake_and_real,y1)\n",
        "\n",
        "            y2 = tf.constant([[1.]]*batch_size) #labels for discriminator\n",
        "\n",
        "            discriminator.trainable = False #stop discriminator training\n",
        "\n",
        "            gan.train_on_batch(noise,y2) #training GAN\n",
        "\n",
        "        display.clear_output(wait=True) #don't want previous output there\n",
        "        generate_and_save_images(generator, epoch+1,seed)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator, epochs, seed)"
      ],
      "id": "5e670cf8-a513-4e8f-89f6-53960513be89"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the DCGAN"
      ],
      "metadata": {
        "id": "UKLt2CLYYIhz"
      },
      "id": "UKLt2CLYYIhz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c732f2fe-03e4-48c4-a1a3-0842ffe15866"
      },
      "outputs": [],
      "source": [
        "x_train_dcgan = x_train.reshape(-1,28,28, 1)*2. -1."
      ],
      "id": "c732f2fe-03e4-48c4-a1a3-0842ffe15866"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e55f4ed2-bf84-458f-bbdc-5e59e9ed1f9a"
      },
      "outputs": [],
      "source": [
        "#create batch from Fashion MNIST\n",
        "batch_size = 32\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x_train_dcgan).shuffle(1000)\n",
        "dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)"
      ],
      "id": "e55f4ed2-bf84-458f-bbdc-5e59e9ed1f9a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "37ebf81c-c43a-4534-964f-3af29e057292"
      },
      "outputs": [],
      "source": [
        "#Training dcgan\n",
        "\n",
        "hist =train_dcgan(gan,dataset,batch_size,num_features,epochs=5)"
      ],
      "id": "37ebf81c-c43a-4534-964f-3af29e057292"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bnm7vo4oH2U"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the discriminator and generator losses\n",
        "plt.suptitle('Loss')\n",
        "plt.plot(hist.history['d_loss'], label='d_loss')\n",
        "plt.plot(hist.history['g_loss'], label='g_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "0bnm7vo4oH2U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a9c3e1a-d69c-430d-9cc1-5923de1c3299"
      },
      "source": [
        "## Generate Synthetic Images with DCGAN"
      ],
      "id": "2a9c3e1a-d69c-430d-9cc1-5923de1c3299"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f7ad038-e885-4ef2-8f30-101ad7015f15"
      },
      "outputs": [],
      "source": [
        "noise = tf.random.normal(shape=[batch_size,num_features])\n",
        "generated_images = generator(noise)\n",
        "\n",
        "show(generated_images,8) #8 is columns"
      ],
      "id": "3f7ad038-e885-4ef2-8f30-101ad7015f15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceac2c72-6da7-48d8-8e90-3f73d3bcc0e2"
      },
      "outputs": [],
      "source": [
        "## Source: https://www.tensorflow.org/tutorials/generative/dcgan#create_a_gif\n",
        "\n",
        "import imageio\n",
        "import glob\n",
        "\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "    filenames = glob.glob('image*.png')\n",
        "    filenames = sorted(filenames)\n",
        "    last = -1\n",
        "    for i,filename in enumerate(filenames):\n",
        "        frame = 2*(i**0.5)\n",
        "        if round(frame) > round(last):\n",
        "            last = frame\n",
        "        else:\n",
        "            continue\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "\n",
        "import IPython\n",
        "display.Image(filename=anim_file)"
      ],
      "id": "ceac2c72-6da7-48d8-8e90-3f73d3bcc0e2"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}